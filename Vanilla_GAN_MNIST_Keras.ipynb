{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GNQ5jQvb8s3"
      },
      "source": [
        "# Vanilla GAN of the MNIST dataset with KERAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "LNpW2x4pcTr5",
        "outputId": "ec71e691-6625-4aa3-9867-e3af94dc4654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.1.1 namex-0.0.7 optree-0.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b2914643b3ae4eff850d023098d95aea",
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install --upgrade keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SqfccNamb8s7",
        "outputId": "05b7e16d-f9e7-41b4-9bb1-1f86d58dc6bf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n",
            "3.1.1\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.core'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-05709547cc4b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(tf.__version__) # Tensorflow 1.1.0\n",
        "print(keras.__version__) # Keras 2.0 is required here (revert to it if needed) otherwise LeakyRelu doesn't work\n",
        "\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers\n",
        "from keras.layers import Activation, Dense\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj7Z6BE6b8s8"
      },
      "outputs": [],
      "source": [
        "# Let Keras know that we are using tensorflow as our backend engine\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "#import random\n",
        "#seed = random.randint(0,9999)\n",
        "#random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8C8jcD6b8s9"
      },
      "source": [
        "### Load MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbP8Utxwb8s9"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "# normalize our inputs to be in the range[-1, 1]\n",
        "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "# convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have 784 columns per row\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "\n",
        "(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'training samples')\n",
        "print(X_test.shape[0], 'test samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J_VFPjbb8s9"
      },
      "source": [
        "### Visualise MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2_zdzRDb8s9"
      },
      "outputs": [],
      "source": [
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(X_train.reshape(X_train.shape[0], 28, 28)[i], interpolation='nearest', cmap='gray_r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eupXZTsb8s-"
      },
      "outputs": [],
      "source": [
        "# We will use the Adam optimizer\n",
        "def get_optimizer():\n",
        "    return Adam(lr=0.0002, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pNv_NU9b8s-"
      },
      "outputs": [],
      "source": [
        "def get_generator(optimizer, random_dim):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(256, input_dim=random_dim, kernel_initializer = 'uniform', bias_initializer = 'zeros'))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    generator.add(Dense(1024))\n",
        "    generator.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    generator.add(Dense(784, activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    generator.summary()\n",
        "    return generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoByzCyPb8s-"
      },
      "outputs": [],
      "source": [
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(1024, input_dim=784, kernel_initializer = 'uniform', bias_initializer = 'zeros'))\n",
        "    discriminator.add(LeakyReLU(alpha=0.3))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(512))\n",
        "    discriminator.add(LeakyReLU(alpha=0.3))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.3))\n",
        "    discriminator.add(Dropout(0.2))\n",
        "\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    discriminator.summary()\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPbu4P_Ob8s_"
      },
      "outputs": [],
      "source": [
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "\n",
        "    # get the output of the discriminator (probability of the image being real or not)\n",
        "    gan_output = discriminator(x)\n",
        "\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output) # inputs and outputs under keras version 2.2.2\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    gan.summary()\n",
        "    return gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIAHHhy5b8s_"
      },
      "source": [
        "###  A function which will save your generated images every 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57FyZ26Rb8s_"
      },
      "outputs": [],
      "source": [
        "# Create a wall of generated MNIST images\n",
        "def plot_generated_images(epoch, generator, random_dim, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6cvQhW6b8s_"
      },
      "source": [
        "### Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGIeRzeQb8s_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train(X_train, y_train, x_test, y_test, epochs=100, minibatch_size=128, random_dim = 100):\n",
        "\n",
        "# Build our GAN network\n",
        "    adam = get_optimizer()\n",
        "    G = get_generator(adam, random_dim)\n",
        "    D = get_discriminator(adam)\n",
        "    gan = get_gan_network(D, random_dim, G, adam)\n",
        "\n",
        "    # for plotting at the end\n",
        "    D_loss = []\n",
        "    G_loss = []\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "\n",
        "        # Defines a cost related to an epoch\n",
        "        epoch_cost = 0.\n",
        "\n",
        "\n",
        "        # get number of minibatch based on size of data\n",
        "        num_minibatches = int(X_train.shape[0] / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "\n",
        "        # Randomize data point\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "\n",
        "        # Split the training data into batches of size 128\n",
        "        for i in range(0, X_train.shape[0], minibatch_size):\n",
        "            #print(i)\n",
        "\n",
        "            # Get pair of (X, y) of the current minibatch\n",
        "            X_train_mini = X_train[i:i + minibatch_size]\n",
        "            y_train_mini = y_train[i:i + minibatch_size]\n",
        "\n",
        "\n",
        "            ##### Train discriminator #####\n",
        "            # Get a set of legit images from MNIST data\n",
        "            legit_images = X_train_mini[np.random.randint(0, X_train_mini.shape[0], size=int(minibatch_size/2))]\n",
        "\n",
        "            # Get a set of fake images generated from noise\n",
        "            noise = np.random.normal(0, 1, size=[int(minibatch_size/2), random_dim]) # random_dim = 100 here\n",
        "            syntetic_images = G.predict(noise)\n",
        "\n",
        "\n",
        "            # create 1 dataset with both legit (1) and generated (0) images\n",
        "            x_combined_batch = np.concatenate((legit_images, syntetic_images))\n",
        "            y_combined_batch = np.concatenate((np.ones((int(minibatch_size/2), 1)), np.zeros((int(minibatch_size/2), 1))))\n",
        "            y_combined_batch[:int(minibatch_size/2)] = 0.9 # only for real images\n",
        "\n",
        "            # Train discriminator\n",
        "            D.trainable = True\n",
        "            d_loss = D.train_on_batch(x_combined_batch, y_combined_batch)\n",
        "            D_loss.append(d_loss)\n",
        "\n",
        "\n",
        "            ###### Train generator #####\n",
        "            noise = np.random.normal(0, 1, size=[minibatch_size, random_dim])\n",
        "            y_gen = np.ones(minibatch_size)\n",
        "            D.trainable = False\n",
        "            g_loss = gan.train_on_batch(noise, y_gen)\n",
        "            G_loss.append(g_loss)\n",
        "\n",
        "\n",
        "        print (\"Cost of D after epoch %i: %f\" % (e, d_loss))\n",
        "        print (\"Cost of G after epoch %i: %f\" % (e, g_loss))\n",
        "\n",
        "\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "\n",
        "\n",
        "            \"\"\"# PLOT examples of 5 first synthetic images created by G\n",
        "            for i in range(5):\n",
        "                plt.subplot(5,5,i+1)\n",
        "                plt.xticks([])\n",
        "                plt.yticks([])\n",
        "                plt.grid(False)\n",
        "                plt.imshow(syntetic_images.reshape(int(minibatch_size/2), 28, 28)[i], interpolation='nearest', cmap='gray_r')\n",
        "            plt.show()\"\"\"\n",
        "\n",
        "            plot_generated_images(e, G, random_dim)\n",
        "\n",
        "    # Save models in case (creates a HDF5 file 'model.h5')\n",
        "    G.save_weights('Generator.h5')\n",
        "    D.save_weights('Discriminator.h5')\n",
        "\n",
        "    return [D_loss, G_loss]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    [D_loss, G_loss] = train(X_train, y_train, X_test, y_test, epochs = 100, minibatch_size=128, random_dim = 100) # random_dim = The dimension of our random noise vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdfVx0cxb8tA"
      },
      "source": [
        "### Visualise loss of D and G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1zvtZk3b8tA"
      },
      "outputs": [],
      "source": [
        "ax = pd.DataFrame({'Generative Loss': G_loss, 'Discriminative Loss': D_loss,}).plot(title='Training loss', logy=True, )\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQvgqV8nb8tA"
      },
      "outputs": [],
      "source": [
        "## Get the trained weights post training\n",
        "\n",
        "\n",
        "#from keras.models import load_weights\n",
        "\n",
        "# Returns a compiled model identical to the previous one\n",
        "adam = get_optimizer()\n",
        "random_dim = 100\n",
        "G1 = get_generator(adam, random_dim)\n",
        "D1 = get_discriminator(adam)\n",
        "\n",
        "\n",
        "G1.load_weights('Generator.h5')\n",
        "D1.load_weights('Discriminator.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:kerasproject]",
      "language": "python",
      "name": "conda-env-kerasproject-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
